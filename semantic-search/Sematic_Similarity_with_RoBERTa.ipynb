{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sematic Similarity with RoBERTa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 簡介\n",
        "語意相似是去判斷輸入的2個句子的意思偏向...想一下商業應用以及指標規劃，讓其完整。"
      ],
      "metadata": {
        "id": "kpFd4VattA2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "JfrJK56qtf2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0gw1pDMsC6f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 設定超參數\n",
        "\n"
      ],
      "metadata": {
        "id": "PZ2MOu_1ttvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "labels = [\"contradiction\", \"entailment\", \"neutral\"]"
      ],
      "metadata": {
        "id": "H_KbYjAgtdxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下載知名的SNLI 資料集並且壓縮\n",
        "\n",
        "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
        "!tar -xvzf data.tar.gz"
      ],
      "metadata": {
        "id": "Y5ZT5eXWt7Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 原始資料集有超過55萬筆資訓練資料，我們只取部分10萬當做訓練\n",
        "\n",
        "train_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n",
        "valid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\n",
        "test_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n",
        "\n",
        "# 輸出資料的形狀\n",
        "print(f\"Total train samples: {train_df.shape}\")\n",
        "print(f\"Total validation samples: {valid_df.shape}\")\n",
        "print(f\"Total test samples: {test_df.shape}\")"
      ],
      "metadata": {
        "id": "tJETjADQt9k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 簡單觀察資料\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "_PXfWETKuncf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df.head()"
      ],
      "metadata": {
        "id": "dA0SwMJMo1te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "yB6gZgMso4AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 看到其similarity 為標籤，之後要轉換成整數\n",
        "- sentence1 以及 sentence2為我們的輸入輸入，因此模型要使用RoBERTa，故會透過[SEP]將其隔開。"
      ],
      "metadata": {
        "id": "S_u4b0Bbu0wN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 資料前處理\n",
        "- 觀察資料缺失\n",
        "- 觀察資料長度\n",
        "- 觀察類別資訊\n",
        "- 將資料包裝成適合的形式，可送入模型"
      ],
      "metadata": {
        "id": "cT1CgDrrvWh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 資料缺失值直接移除\n",
        "\n",
        "print(\"遺失的資料比數\")\n",
        "print(train_df.isnull().sum())          # 會將每一個欄位的缺失比數顯示\n",
        "train_df.dropna(axis=0, inplace=True)   # 將缺失值移除，且是inplace執行"
      ],
      "metadata": {
        "id": "qaiJddnGux9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 類別資訊比例(訓練)\n",
        "\n",
        "train_df['similarity'].value_counts()"
      ],
      "metadata": {
        "id": "DJiWsjWBwGeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 類別資訊比例(測試)\n",
        "\n",
        "valid_df['similarity'].value_counts()"
      ],
      "metadata": {
        "id": "TdQe6Mkfok2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 比例是差不多的，就不多做處理。\n",
        "\n",
        "但有-的類別，我們將其移除。"
      ],
      "metadata": {
        "id": "L8hEY5D9o-QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 移除-類別\n",
        "\n",
        "train_df = train_df[train_df['similarity'] != '-']\n",
        "valid_df = valid_df[valid_df['similarity'] != '-']\n",
        "test_df = test_df[test_df['similarity'] != '-']"
      ],
      "metadata": {
        "id": "aTKz8TYFoyDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 將類別轉成數值，有兩種方法，直接轉換為index，或者one-hot encoding，這裡用index就好\n",
        "mapping = {\n",
        "    'neutral': 0,\n",
        "    'contradiction': 1,\n",
        "    'entailment': 2,\n",
        "    0: 'neutral',\n",
        "    1: 'contradiction',\n",
        "    2: 'entailment'\n",
        "}\n",
        "\n",
        "\n",
        "train_df[\"label\"] = train_df[\"similarity\"].apply(\n",
        "    lambda x: mapping[x]\n",
        ")\n",
        "# y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n",
        "y_train = train_df.label.values\n",
        "\n",
        "valid_df[\"label\"] = valid_df[\"similarity\"].apply(\n",
        "    lambda x: mapping[x]\n",
        ")\n",
        "# y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n",
        "y_val = valid_df.label.values\n",
        "\n",
        "test_df[\"label\"] = test_df[\"similarity\"].apply(\n",
        "    lambda x: mapping[x]\n",
        ")\n",
        "# y_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)\n",
        "y_test = test_df.label.values"
      ],
      "metadata": {
        "id": "59XKy3g1phBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 觀看label\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "Xlhmfl3RqP1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "兩個句子連接的tokenizer"
      ],
      "metadata": {
        "id": "Vdzd4labyhpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
        "tokenizer.encode_plus('You are my sunshine.', 'You are my heroes')"
      ],
      "metadata": {
        "id": "begTCIscykxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立一個 generator: 繼承 Sequence, https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
        "\n",
        "\n",
        "class BertSemanticDataGenerator(tf.keras.utils.Sequence):       # 至少要override __len__, __getitem__\n",
        "    \"\"\"\n",
        "        產生 batch of data.\n",
        "        \n",
        "\n",
        "        Args:\n",
        "            sentence_pairs: list/array, of s pairs.\n",
        "            labels: list/array, of labels.\n",
        "            batch_size: integer.\n",
        "            shuffle: boolean, whether to shuffle the data.\n",
        "            include_targets: boolean, whether to include the targets\n",
        "        \n",
        "\n",
        "        Returns:\n",
        "            Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
        "            (or just `[input_ids, attention_mask, `token_type_ids]`\n",
        "            if `include_targets=False`)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        sentence_pairs,\n",
        "        labels,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        include_targets=True\n",
        "    ):\n",
        "        self.sentence_pairs = sentence_pairs\n",
        "        self.labels = labels\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.include_targets = include_targets\n",
        "\n",
        "        self.tokenizer = transformers.RobertaTokenizer.from_pretrained(\n",
        "            \"roberta-base\", do_lower_case=True\n",
        "        )\n",
        "        self.indexes = np.arange(len(self.sentence_pairs))\n",
        "        self.on_epoch_end()         # Method called at the end of every epoch.\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence_pairs) // self.batch_size\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        index = self.indexes[idx*self.batch_size: (idx+1)*self.batch_size]\n",
        "        sentence_pairs = self.sentence_pairs[self.indexes]\n",
        "\n",
        "        encoded = self.tokenizer.batch_encode_plus(\n",
        "            sentence_pairs.tolist(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"tf\",\n",
        "        )\n",
        "\n",
        "        input_ids = np.array(encoded['input_ids'], dtype='int32')\n",
        "        attention_masks = np.array(encoded['attention_mask'], dtype='int32')\n",
        "        token_type_ids = np.array(encoded['token_type_ids'], dtype='int32')\n",
        "\n",
        "        if self.include_targets:\n",
        "            labels = np.array(self.labels[index], dtype='int32')\n",
        "            return [input_ids, attention_masks, token_type_ids], labels\n",
        "        else:\n",
        "            return [input_ids, attention_masks, token_type_ids]\n",
        "    \n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.RandomState(41).shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "tG3q6pXRqSdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "建立模型"
      ],
      "metadata": {
        "id": "nAvK_Wnjv0B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model under a distribution strategy scope.\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    # Encoded token ids from BERT tokenizer.\n",
        "    input_ids = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        "    )\n",
        "    # Attention masks indicates to the model which tokens should be attended to.\n",
        "    attention_masks = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        "    )\n",
        "    # Token type ids are binary masks identifying different sequences in the model.\n",
        "    token_type_ids = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        "    )\n",
        "    # Loading pretrained BERT model.\n",
        "    bert_model = transformers.TFRobertaModel.from_pretrained(\"roberta-base\")\n",
        "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
        "    bert_model.trainable = False\n",
        "\n",
        "    bert_output = bert_model(\n",
        "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        "    )\n",
        "    sequence_output = bert_output.last_hidden_state     # 全部\n",
        "    pooled_output = bert_output.pooler_output           # [CLS]\n",
        "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "    bi_lstm = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "    )(sequence_output)\n",
        "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
        "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "    model = tf.keras.models.Model(\n",
        "        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"acc\"],\n",
        "    )\n",
        "\n",
        "\n",
        "print(f\"Strategy: {strategy}\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uP7_QrMjvypS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 包裝好訓練資料\n",
        "\n",
        "train_data = BertSemanticDataGenerator(\n",
        "    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "valid_data = BertSemanticDataGenerator(\n",
        "    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
        "    y_val,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "2TnI8tNFwz3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認資料無誤\n",
        "\n",
        "for x in train_data:\n",
        "    print(x)\n",
        "    break"
      ],
      "metadata": {
        "id": "Qs2OGBtVyLmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練模型"
      ],
      "metadata": {
        "id": "zZnY7Hr11TR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.logging.set_verbosity_error()      # 避免: Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=valid_data,\n",
        "    epochs=epochs,\n",
        "    use_multiprocessing=True,\n",
        ")"
      ],
      "metadata": {
        "id": "bvel9X_1waE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning\n",
        "This step must only be performed after the feature extraction model has been trained to convergence on the new data.\n",
        "\n",
        "This is an optional last step where bert_model is unfreezed and retrained with a very low learning rate. This can deliver meaningful improvement by incrementally adapting the pretrained features to the new data."
      ],
      "metadata": {
        "id": "nLhM9RlX1efy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the bert_model.\n",
        "bert_model.trainable = True\n",
        "# Recompile the model to make the change effective.\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3x1T5yAVx2fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuning the model"
      ],
      "metadata": {
        "id": "uGYgsLuX1lZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=valid_data,\n",
        "    epochs=epochs,\n",
        "    use_multiprocessing=True,\n",
        "    workers=-1,\n",
        ")"
      ],
      "metadata": {
        "id": "_ddqOJbm1jSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "準備預測資料"
      ],
      "metadata": {
        "id": "_4YMoa3L1sXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = BertSemanticDataGenerator(\n",
        "    test_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
        "    y_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "model.evaluate(test_data, verbose=1)"
      ],
      "metadata": {
        "id": "xtCOwQb71pnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "真正推論時使用的function"
      ],
      "metadata": {
        "id": "ZeDQU_x_2Hq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_similarity(sentence1, sentence2):\n",
        "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
        "    test_data = BertSemanticDataGenerator(\n",
        "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "    )\n",
        "\n",
        "    proba = model.predict(test_data[0])[0]\n",
        "    idx = np.argmax(proba)\n",
        "    proba = f\"{proba[idx]: .2f}%\"\n",
        "    pred = labels[idx]\n",
        "    return pred, proba"
      ],
      "metadata": {
        "id": "sbuPGj6z1udh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"Two women are observing something together.\"\n",
        "sentence2 = \"Two women are standing with their eyes closed.\"\n",
        "check_similarity(sentence1, sentence2)"
      ],
      "metadata": {
        "id": "jqnGcW0Y1_z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"A smiling costumed woman is holding an umbrella\"\n",
        "sentence2 = \"A happy woman in a fairy costume holds an umbrella\"\n",
        "check_similarity(sentence1, sentence2)"
      ],
      "metadata": {
        "id": "vojHadkV2BFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"A soccer game with multiple males playing\"\n",
        "sentence2 = \"Some men are playing a sport\"\n",
        "check_similarity(sentence1, sentence2)"
      ],
      "metadata": {
        "id": "bXhxMcVf2EYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "至此完成相似句子Roberta模型"
      ],
      "metadata": {
        "id": "suTXHX052L0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9B5qPwTy2S9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}