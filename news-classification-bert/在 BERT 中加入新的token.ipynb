{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 BERT 模型中加入新的token\n",
    "原因\n",
    "- 因為BERT預訓練的場景可能與下游任務不同，為了更符合我們的下游任務，可以加入該領域的token幫助進一步優化embedding，提升模型效果。\n",
    "步驟\n",
    "1. 透過add_token加入新的token於tokenizer，但此時模型還沒有擴充其embedding table\n",
    "2. 透過resize_token_embedding 通知模型，更新embedding table大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:28:19.918152Z",
     "start_time": "2021-12-25T13:28:10.138145Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:29:52.206518Z",
     "start_time": "2021-12-25T13:29:52.187002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CO', '##VI', '##D']\n",
      "['hospital', '##ization']\n"
     ]
    }
   ],
   "source": [
    "# 英文版本\n",
    "\n",
    "print(tokenizer.tokenize('COVID'))\n",
    "print(tokenizer.tokenize('hospitalization'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:36:17.898416Z",
     "start_time": "2021-12-25T13:36:17.890417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "28996\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(len(tokenizer))  # vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以發現COVID 如果我們把其想當作一個token，我們可以去新增!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:36:31.184614Z",
     "start_time": "2021-12-25T13:36:30.830625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我們新增的 2 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(28998, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added_toks = tokenizer.add_tokens(['COVID', 'hospitalization'])\n",
    "print('我們新增的', num_added_toks, 'tokens')\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以看到已經新增了新的token, vocab_size+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:37:22.011867Z",
     "start_time": "2021-12-25T13:37:22.003868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以看到已經完成了!\n",
    "tokenizer.tokenize('COVID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**接下來嘗試中文的部分**\n",
    "- 假設我要新增一些domain-specific 的token\n",
    "    - 政大經濟研究所"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:40:41.502057Z",
     "start_time": "2021-12-25T13:40:31.896028Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:41:46.790330Z",
     "start_time": "2021-12-25T13:41:46.771298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我',\n",
       " '是',\n",
       " '一',\n",
       " '個',\n",
       " '資',\n",
       " '料',\n",
       " '科',\n",
       " '學',\n",
       " '家',\n",
       " '，',\n",
       " '生',\n",
       " '活',\n",
       " '在',\n",
       " '台',\n",
       " '北',\n",
       " '，',\n",
       " '就',\n",
       " '讀',\n",
       " '政',\n",
       " '大',\n",
       " '經',\n",
       " '濟',\n",
       " '研',\n",
       " '究',\n",
       " '所']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('我是一個資料科學家，生活在台北，就讀政大經濟研究所')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:44:57.880093Z",
     "start_time": "2021-12-25T13:44:57.596098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我們新增的 2 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(21130, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added_toks = tokenizer.add_tokens(['政大經濟研究所', '資料科學家'])\n",
    "print('我們新增的', num_added_toks, 'tokens')\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-25T13:46:10.067681Z",
     "start_time": "2021-12-25T13:46:10.048690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我',\n",
       " '是',\n",
       " '一',\n",
       " '個',\n",
       " '資',\n",
       " '料',\n",
       " '科',\n",
       " '學',\n",
       " '家',\n",
       " '，',\n",
       " '生',\n",
       " '活',\n",
       " '在',\n",
       " '台',\n",
       " '北',\n",
       " '，',\n",
       " '就',\n",
       " '讀',\n",
       " '政',\n",
       " '大',\n",
       " '經',\n",
       " '濟',\n",
       " '研',\n",
       " '究',\n",
       " '所']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('我是一個資料科學家，生活在台北，就讀政大經濟研究所')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以看到中文雖然tokenizer加入了新的token，但沒有發生效果，這是因為BERT 中文是用字為單位，所以基本上是不會有出現沒看過的字的情況，故比較不需要此種應用\n",
    "\n",
    "- 不過有人建立了 word-based BERT，詳細可以[參考](https://www.jiqizhixin.com/articles/2020-09-25-2)\n",
    "    - 也有比較其優劣之處"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
