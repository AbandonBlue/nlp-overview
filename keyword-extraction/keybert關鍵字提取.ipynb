{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWRsjCtNloKd"
   },
   "source": [
    "## [keyBERT](https://maartengr.github.io/KeyBERT/)\n",
    "- [自己換模型](https://www.sbert.net/docs/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKuASwEWhhBa",
    "outputId": "c2efe44e-b7d3-4349-8432-de425a706a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading https://files.pythonhosted.org/packages/db/fb/822e7094457cd16319291e34aa97b2ef97620da01af94fe557e96a8cc6b9/keybert-0.3.0.tar.gz\n",
      "Collecting sentence-transformers>=0.3.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/75/df441011cd1726822b70fbff50042adb4860e9327b99b346154ead704c44/sentence-transformers-1.2.0.tar.gz (81kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 11.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.19.5)\n",
      "Collecting transformers<5.0.0,>=3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/92/6153f4912b84ee1ab53ab45663d23e7cf3704161cb5ef18b0c07e207cef2/transformers-4.7.0-py3-none-any.whl (2.5MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5MB 38.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.41.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.8.1+cu101)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.9.1+cu101)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.4.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 43.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.12)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 50.3MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 56.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (20.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (4.5.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
      "Building wheels for collected packages: keybert, sentence-transformers\n",
      "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keybert: filename=keybert-0.3.0-cp37-none-any.whl size=19436 sha256=b7fe8b4baea679def024794a91e4b2dfcf0a66729c3ca1c855959dd915666861\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/ab/2b/bc129895ee956866333f364f1b3beb20efa788e074c03e6b34\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.0-cp37-none-any.whl size=123339 sha256=b7d1b248a7e5c8fef4962ce70a4609b4322bd0d3fa07fe1ceccbe3a7fc109687\n",
      "  Stored in directory: /root/.cache/pip/wheels/0f/06/f7/faaa96fdda87462b4fd5c47b343340e9d5531ef70d0eef8242\n",
      "Successfully built keybert sentence-transformers\n",
      "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers, keybert\n",
      "Successfully installed huggingface-hub-0.0.8 keybert-0.3.0 sacremoses-0.0.45 sentence-transformers-1.2.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4qb5pYBieCk"
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152,
     "referenced_widgets": [
      "7b32383d01fe4695aecd70a4329dfc17",
      "138b5b358c4843779f9fe6e06a056aa0",
      "ae9402350f5643c583434f08925377e6",
      "5357ebb9024f41d9bc733a5c529257f6",
      "40763304fcb745ea9c08ac1e5c80580f",
      "a347223aaf374e5692cb5ab572fdc91e",
      "f14034beff874cb194445e6a4928c016",
      "38f2d80a2808438bb9fe16cca2f03d62"
     ]
    },
    "id": "y_32gYFLhp3J",
    "outputId": "76859389-edad-40e9-ab4e-77eb82c02224"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b32383d01fe4695aecd70a4329dfc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244733649.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('blogpost', 0.2089),\n",
       " ('python', 0.1923),\n",
       " ('tutorials', 0.1719),\n",
       " ('macbook', 0.1712),\n",
       " ('thursday', 0.1369)]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"\"\"\n",
    "Towards Data Science\n",
    "Sign in\n",
    "\n",
    "Get started\n",
    "Follow\n",
    "549K Followers\n",
    "·\n",
    "Editors' Picks\n",
    "Features\n",
    "Deep Dives\n",
    "Grow\n",
    "Contribute\n",
    "About\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You have 1 free member-only story left this month. Sign up for Medium and get an extra one\n",
    "\n",
    "How to Extract Relevant Keywords with KeyBERT\n",
    "Yet another application of BERT\n",
    "Ahmed Besbes\n",
    "Ahmed Besbes\n",
    "\n",
    "1 day ago·5 min read\n",
    "\n",
    "\n",
    "\n",
    "Image by the author\n",
    "There are many powerful techniques that perform keywords extraction (e.g. Rake, YAKE!, TF-IDF). However, they are mainly based on the statistical properties of the text and don’t necessarily take into account the semantic aspects of the full document.\n",
    "KeyBERT is a minimal and easy-to-use keyword extraction technique that aims at solving this issue. It leverages the BERT language model and relies on the 🤗transformers library.\n",
    "\n",
    "source: https://github.com/MaartenGr/KeyBERT\n",
    "KeyBERT is developed and maintained by \n",
    "Maarten Grootendorst\n",
    ". So go check his repo (and clone it) if you’re interested in using it.\n",
    "In this post, I’ll briefly present KeyBERT: how it works and how you can use it\n",
    "PS: If you want to see a video tutorial on how to use KeyBERT and how to embed it in a Streamlit app, you can have a look at my video:\n",
    "\n",
    "Video by the author\n",
    "KeyBERT: a BERT-powered keyword extraction technique\n",
    "You can install KeyBERT with pip.\n",
    "pip install keybert\n",
    "If you need embeddings from other sources than 🤗transformers, you can install them as well:\n",
    "pip install keybert[flair]\n",
    "pip install keybert[gensim]\n",
    "pip install keybert[spacy]\n",
    "pip install keybert[use]\n",
    "Calling KeyBERT is straightforward: you initialize a keyword extraction model based on a 🤗transformers model and apply the extract_keywords method on it.\n",
    "\n",
    "source: https://github.com/MaartenGr/KeyBERT\n",
    "How does KeyBERT extract keywords?\n",
    "KeyBERT extracts keywords by performing the following steps:\n",
    "1 — The input document is embedded using a pre-trained BERT model. You can pick any BERT model your want from 🤗transformers. This turns a chunk of text into a fixed-size vector that is meant the represent the semantic aspect of the document\n",
    "2 — Keywords and expressions (n-grams) are extracted from the same document using Bag Of Words techniques (such as a TfidfVectorizer or CountVectorizer). This is a classical step that you may be familiar with if you’ve performed keywords extraction in the past\n",
    "\n",
    "Image by the author\n",
    "3 — Each keyword is then embedded into a fixed-size vector with the same model used to embed the document\n",
    "\n",
    "Image by the author\n",
    "4 — Now that the keywords and the document are represented in the same space, KeyBERT computes a cosine similarity between the keyword embeddings and the document embedding. Then, the most similar keywords (with the highest cosine similarity score) are extracted.\n",
    "\n",
    "Image by the author\n",
    "The idea is pretty simple: you can think of it as an enhanced version of a classical keyword extraction technique in which the BERT language model comes in to add its semantic capability.\n",
    "This doesn’t stop here: KeyBERT includes two methods to introduce diversity in the resulting keywords.\n",
    "1 — Max Sum Similarity (MSS)\n",
    "To use this method, you start by setting the top_n argument to a value, say 20. Then 2 x top_n keywords are extracted from the document. Pairwise similarities are computed between these keywords. Finally, the method extracts the most relevant keywords that are the least similar to each other.\n",
    "Here’s an example from the KeyBERT’s repository:\n",
    "\n",
    "source: https://github.com/MaartenGr/KeyBERT\n",
    "2 — Maximal Marginal Relevance (MMR)This method is similar to the previous one: it adds a diversity argument\n",
    "MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks.\n",
    "It starts by selecting the keywords that are the most similar to the document. Then, it iteratively selects new candidates that are both similar to the document and not similar to the already selected keywords\n",
    "You can choose a low-diversity threshold:\n",
    "\n",
    "source: https://github.com/MaartenGr/KeyBERT\n",
    "or a high one:\n",
    "\n",
    "source: https://github.com/MaartenGr/KeyBERT\n",
    "So far so good, but…\n",
    "One limitation that KeyBERT may suffer from though is the execution time: if you have large documents and need real-time results, KeyBERT may not be the best solution (unless you have dedicated GPUs in your production environment). The reason being that BERT models are notoriously huge and consume a lot of resources especially when they have to process large documents.\n",
    "You can probably find some hacks to speed up the inference time by picking smaller models (DistilBERT), using mixed precision or even convert your model to ONNX format.\n",
    "If this still doesn’t work out for you, check other classical methods: you’d be surprised by their efficiency despite their relative simplicity.\n",
    "Thanks for reading!\n",
    "That’s it for today. I hope you’ll find this small method useful for your NLP projects if you’re performing keywords extraction.\n",
    "You can learn more about KeyBERT here:\n",
    "MaartenGr/KeyBERT\n",
    "KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and…\n",
    "github.com\n",
    "\n",
    "Keyword Extraction with BERT\n",
    "A minimal method for extracting keywords and keyphrases\n",
    "towardsdatascience.com\n",
    "\n",
    "and here:\n",
    "Self-Supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling\n",
    "In this paper we propose a novel self-supervised approach of keywords and keyphrases retrieval and extraction by an…\n",
    "www.preprints.org\n",
    "\n",
    "Take care,\n",
    "Sign up for The Variable\n",
    "By Towards Data Science\n",
    "Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.\n",
    "\n",
    "Get this newsletter\n",
    "You'll need to sign in or create an account to receive this newsletter.\n",
    "\n",
    "46\n",
    "\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "Data Science\n",
    "NLP\n",
    "Bert\n",
    "Transformers\n",
    "Machine Learning\n",
    "More from Towards Data Science\n",
    "Follow\n",
    "Your home for data science. A Medium publication sharing concepts, ideas and codes.\n",
    "\n",
    "Antony Henao\n",
    "\n",
    "·1 day ago\n",
    "\n",
    "Data Engineers Shouldn’t Write Airflow Dags — Part 2\n",
    "A framework proposal for Apache Airflow\n",
    "\n",
    "Photo by Richard Horvath on Unsplash\n",
    "This is the second article about why Data Engineers shouldn’t write Airflow DAGs. In this new article, we are going to introduce a framework proposal for Apache Airflow.\n",
    "This article aims to shed some light on how building a framework can help you solve some of the problems related to DAG writing.\n",
    "In this new article, I’m going to do a short recap about the first part. So, it is not necessary to read it. Though, you should consider reading it if you want a more detailed explanation of the things I’m going to address in this one.\n",
    "Data Engineers Shouldn’t Write Airflow Dags — Part 1\n",
    "towardsdatascience.com\n",
    "\n",
    "Why Data Engineer shouldn’t write DAGs\n",
    "As I…\n",
    "Read more · 7 min read\n",
    "\n",
    "88\n",
    "\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "Bilal Mahmood Khan\n",
    "\n",
    "·1 day ago\n",
    "\n",
    "Calculating Internal Rate of Return (IRR) in BigQuery\n",
    "A step-by-step guide to implementing Excel’s IRR function in BigQuery\n",
    "Internal Rate of Return (IRR) is a common calculation that often comes up in finance. In this blogpost, I will show how to build a query in BigQuery that carries out the calculation equivalent to Excel’s IRR function. This is helpful if the cashflows are stored in a BigQuery table and you want to calculate IRR without moving the data to Excel, or if you want to use BigQuery as a source for data visualizations and BI.\n",
    "\n",
    "Photo by Scott Graham on Unsplash\n",
    "1. Background: Internal Rate of Return (IRR) and Net Present Value (NPV)\n",
    "Internal Rate of Return (IRR) is the rate of return at which the Net Present Value of all cashflows resulting from an investment…\n",
    "Read more · 6 min read\n",
    "\n",
    "30\n",
    "\n",
    "\n",
    "\n",
    "Dea Bardhoshi\n",
    "\n",
    "·1 day ago\n",
    "\n",
    "Healthiest Cities in the US: Part 1, Food\n",
    "How healthy are US cities?\n",
    "\n",
    "Photo by Haseeb Jamil on Unsplash\n",
    "I have been thinking about using data science and statistics in the context of urban analysis, especially about the question of how healthy US cities are. In these multi-part stories, we are going to be analysing this question using data science. At this moment, I plan on dividing the stories like this: food, recreational spaces, and work-life balance. For this first part, we are going to be focusing on food: namely how common fast food chains are throughout American cities, and how accessible grocery bought food is to the American people. Let’s get started:\n",
    "Data:\n",
    "Fast Food Data: https://www.kaggle.com/datafiniti/fast-food-restaurants\n",
    "Tools:\n",
    "Jupyter Notebooks\n",
    "…\n",
    "Read more · 7 min read\n",
    "\n",
    "26\n",
    "\n",
    "\n",
    "\n",
    "Mateusz Kwaśniak\n",
    "\n",
    "·1 day ago\n",
    "\n",
    "Kubeflow (is not) for Dummies\n",
    "Deploy and destroy Kubeflow on EKS with one script, no sweat\n",
    "\n",
    "Figure 1. Kubeflow Dashboard (Source: Kubeflow docs)\n",
    "Tools, libraries, frameworks are created to make our work easier. They introduce new functionalities, simplify code, reduce boilerplate, automate stuff.\n",
    "Imagine your project with no dependencies, imagine you need to replace a single function call (e.g. yaml.safe_load) with your own piece of code for that functionality. All these tools make applications easier to build and maintain, develop and deploy. But what if these tools themselves are difficult to be deployed? Ouch.\n",
    "Introducing Kubeflow\n",
    "I won’t lie — there’s no coincidence that I introduce Kubeflow right after writing that tools can be difficult to be deployed. …\n",
    "Read more · 5 min read\n",
    "\n",
    "23\n",
    "\n",
    "\n",
    "\n",
    "Harsh Maheshwari\n",
    "\n",
    "·1 day ago\n",
    "\n",
    "WANT TO LEARN SQL QUICKLY?\n",
    "Learn Essential SQL Skills for Data Scientists in 5 Minutes\n",
    "Learn essential SQL skills to make your data science resume more attractive\n",
    "\n",
    "Photo by Joshua Sortino on Unsplash\n",
    "Let’s start with an interesting fact. SQL is the most required skill for data analysts and data engineers and the third most required skill for data scientists. Well, this fact simply tells you how important it is to know SQL.\n",
    "Okay, but why is SQL a highly desired skill set? Usually, data scientists work with data frames in Python or R. However, the vast amount of data in today’s data science can’t be completely loaded into a data frame or even into a .csv file. For such cases, an SQL database is required. SQL is very simple and easy to…\n",
    "Read more · 6 min read\n",
    "\n",
    "121\n",
    "\n",
    "\n",
    "\n",
    "Read more from Towards Data Science\n",
    "More From Medium\n",
    "What’s New in Python 3.10?\n",
    "James Briggs in Towards Data Science\n",
    "\n",
    "6 Python Projects You Can Finish in a Weekend\n",
    "Frank Andrade in Towards Data Science\n",
    "\n",
    "3 Awesome Python Libraries That You Should Know About\n",
    "Ismael Araujo in Towards Data Science\n",
    "\n",
    "20 Lessons learned going from Junior Data Scientist to Chief Data Scientist\n",
    "Mathias Gruber in Towards Data Science\n",
    "\n",
    "Why You’ll Quit Your Data Science Job\n",
    "Vicky Yu in Towards Data Science\n",
    "\n",
    "Dual Boot is Dead: The Post Mortem\n",
    "Dimitris Poulopoulos in Towards Data Science\n",
    "\n",
    "M1 MacBook Pro vs. Intel i9 MacBook Pro — Ultimate Data Science Comparison\n",
    "Dario Radečić in Towards Data Science\n",
    "\n",
    "Data Scientists and ML Engineers Are Luxury Employees\n",
    "Adrien Biarnes in Towards Data Science\n",
    "\n",
    "About\n",
    "\n",
    "Help\n",
    "\n",
    "Legal\n",
    "\"\"\"\n",
    "\n",
    "kw_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "keywords = kw_model.extract_keywords(doc)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNHUC1Kziao1",
    "outputId": "9f336d77-a3a7-4149-fe48-8ff0a936a787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('projects finish weekend', 0.1414),\n",
       " ('macbook pro ultimate', 0.4011),\n",
       " ('science hands tutorials', 0.1785),\n",
       " ('549k followers editors', 0.2469),\n",
       " ('medium new python', -0.0041)]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSS方法\n",
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', \n",
    "                              use_maxsum=True, nr_candidates=20, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvpY7BZzjNCQ",
    "outputId": "c44e69cf-c2fd-451d-d558-6d7d415555e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science 20 lessons', 0.507),\n",
       " ('writing tools difficult', 0.0996),\n",
       " ('grocery bought food', 0.2161),\n",
       " ('github com keyword', 0.3079),\n",
       " ('airflow dags new', 0.2573)]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MMR\n",
    "\n",
    "kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', \n",
    "                              use_mmr=True, diversity=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhnsi-h_mJvy"
   },
   "source": [
    "### 中文版本\n",
    "- 使用 distiluse-base-multilingual-cased-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQwfMNtqkD_D"
   },
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "    建立資料科學團隊\n",
    "隨著資料科學在各領域的開發、應用持續升溫，建立專業分工的團隊是必經之路。資料科學團隊的建立會面臨著不一樣的挑戰，讓我們跟著 Wendy 了解其在 Kaggle 面臨的挑戰。\n",
    "活動主辦單位：Taiwan Data Science Meetup 台灣資料科學社群\n",
    "大綱：\n",
    "一、介紹 (Introduction)\n",
    "二、資料基礎設施 (Data infrastructure)\n",
    "三、分析系統 (Analytics systems)\n",
    "四、機器學習 (Machine learning)\n",
    "五、Q&A\n",
    "一、介紹 (Introduction)\n",
    "\n",
    "講者: Wendy Kan\n",
    "講者介紹：\n",
    "在生醫工程跟醫學ML界打滾幾年後，Wendy 在 Kaggle 做了四年的資料科學家，主要負責設計ML比賽。過去兩年間，Wendy 在 Kaggle 成立了一個小Data team。Wendy 將會分享從零開始成立一個小Data team遇到的挑戰。Wendy 目前已經離開 Kaggle 加入了Google Ads AI team，不過仍然與Wendy待了六年的 Kaggle 有友好的合作關係。\n",
    "二、資料基礎設施 (Data infrastructure)\n",
    "挑戰：散落在各處的資料 (Data everywhere)\n",
    "原因\n",
    "- Kaggle的資料分析團隊在一開始，資料基礎設施不佳，資料來源非常複雜，發現許多人在做相同的系統建立上(如join不同來源的資料)，資料成長得非常快，會有發生crash的風險。\n",
    "不同的資料來源分析過程(原先)\n",
    "- Back End Analytics(MSSQL DB): 從資料庫透過SQL Explorer / Local(or GCP) MSSQL connector 去取得資料轉換成kaggle Dataset並在kernels中做分析。\n",
    "- Front End Analytics: 從前端網頁埋點觀察使用者行為收集的資料庫，透過BigQuery / Data Studio取得、使用資料去做分析。\n",
    "- Google Analytics(較少用，忽略)\n",
    "透過ETL整合不同的資料來源\n",
    "開發時程:\n",
    "- 2個月設計\n",
    "- 3個月實作\n",
    "- 2個月資料隱私review\n",
    "系統架構:\n",
    "Data Source(Back End Analytics/Front End Analytics/Google Analytics) → ETL(v1: Airflow, v2: Custom) → Data Warehouse → 應用場域(kaggle note books/BigQuery/Dashboard/Kaggle Datasets)\n",
    "成效:\n",
    "- 公司內部的任何人都可以透過Data Warehouse簡易地取得想獲取的資料，改善了原先大家重複地從不同資料源整理合併地過程。\n",
    "- 甚至可以透過簡單的BigQuery UI直接獲取資料。\n",
    "4. Lessons learned\n",
    "基礎設施是資料科學團隊的基礎\n",
    "- Hacking: 可以去創建一次性的解法，但要設立停損點去建立基礎設施，如: 看到大家不斷在做相同的事情(整合資料)。\n",
    "- Scale: 越早考慮越好，資料會成長得比想像中快，若設計不好，可能會產生crash風險。\n",
    "- Vision: 用最小的努力去達到最大的成效，因此，建立的系統優先解決大多數人最主要的痛點。\n",
    "避免客製化軟體，善用現成工具\n",
    "尤其在小公司，維護的時間成本特別高，不論是處理各資料庫的交互等等問題，若可以使用現有穩定的工具，就去使用。\n",
    "- 使用開源工具或者是付費的雲端工具。\n",
    "- 試著不要客製化data connectors 以及 ETL系統。\n",
    "- 維護客製化的軟體非常痛苦，尤其是小公司。\n",
    "三、分析系統 (Analytics systems)\n",
    "挑戰：基礎設施建立之後，太多分析需求需要處理，如何制定優先順序以及解決方案。\n",
    "原因\n",
    "- 不同部門的需求眾多如: 有多少TensorFlow vs PyTorch個別使用者? 有多少使用者在Fortune500大公司? 可以建立Dashboard嗎? 可以建立一個分析並產出報表嗎?\n",
    "- 在眾多需求之下，資料分析部門需要排列優先順序以及必要性，對於公司的文化也開始真正的轉變。\n",
    "分析優先 (Analytics first)\n",
    "分析的CP值非常高，雖然我們都想要做ML，但有的時候分析就足夠去解決大部分的問題。\n",
    "\n",
    "- Drive value, deliver insights, build intuition\n",
    "a. Superpower:\n",
    "從資料中取得動見，驅動/支持商業決策。\n",
    "b. Build data intuition for ML definition:\n",
    "因為許多做ML的人員其實專注於技術本身，但並沒有妥善地結合商業價值，許多的專案最後是沒有產生impact，因此從分析中建立對商業問題、價值的理解是必不可少的。\n",
    "\n",
    "- Determine centralized or distributed Data Science team\n",
    "a. Centralized:\n",
    "將資料科學團隊壯大，專門去處理各部門的分析問題。\n",
    "b. Distributed:\n",
    "透過諮詢、分享，如給一個簡單的例子讓部門的人學習，將分析的文化植入各部門，讓各部門的人也可以有分析能力，也是 Wendy 在 Kaggle 時採用的方法。\n",
    "\n",
    "- Give people freedom to choose tools\n",
    "a. Can always consolidate later:\n",
    "讓大家多去使用系統為優先，先建立起data-driven的文化。\n",
    "\n",
    "- Analytics or ML?\n",
    "a. Be patient. ML system is hard! ML system takes time to define, build and to tune\n",
    "b. Deliver analytics results quickly\n",
    "c. In the meantime, you lay the groundwork for ML (usually infrastructure is shared)\n",
    "分析系統 (Analytics systems)\n",
    "\n",
    "Analytics systems\n",
    "4. 例子 (A workflow example)\n",
    "\n",
    "Workflow example\n",
    "在建立了基礎設施之後，原先需要專門一個工程師做的分析工作(更新Kaggle 最常用的package到kernel上)，可以透過 BigQuery 去自動在雲端運行，節省了以往人工的部分。\n",
    "四、機器學習 (Machine learning)\n",
    "挑戰：一個問題通常都有許多解決方法，你需要去找到證據去支持現有的方法不夠好，需要去建立ML方法，在途中遇到的挑戰。\n",
    "為什麼不使用heuristics (Why don’t we use heuristics?)\n",
    "- 在做ML的時候，一定會有很多人問到為什麼需要ML?為什麼不等一段時間得到真實資料後去做統計量當做value?(heuristics)\n",
    "- 或者為什麼要選擇建立一個需要時時維護、更新以及監控的ML系統，需要大量的資料標籤以及其他工程?\n",
    "Lessons learned\n",
    "- It’s okay to use heuristics:\n",
    "a. 首先一定要考慮ML的必要性，如果沒有資料就先去取得資料或者考慮取得的成本與效益是否合理。\n",
    "b. 先用heuristics方法去解決問題，看結果是否已經足夠好了，再決定是否繼續。\n",
    "\n",
    "- 需要做一些實驗確認需要ML去解決問題 (It takes some experiment to show you really do need ML):\n",
    "a. 先做 PoC 去嘗試，確保有是潛力的。\n",
    "b. 保持 PoC 簡單，不論是演算法或者是系統。\n",
    "c. 為了速度、效能以及使用者體驗，先用現存的ML服務去建構PoC是完全沒問題的。可以在確定有潛力之後，再建立更客製化的服務。\n",
    "\n",
    "(補: 可以透過 參加比賽(kaggle) 訓練快速鍵立prototype的能力)\n",
    "\n",
    "- 如何去說服公司需要ML (How to convince people?)\n",
    "a. 從領導者/決策者的觀點出發去思考。\n",
    "b. 提出細節的分析，透過成本/收益的資料分析佐證來說服投資是值得的。\n",
    "找到適合的題目去預測 (Picking a “thing” to predict)\n",
    "- 在找尋10餘個題目之後，成功實現2個\n",
    "\n",
    "- 其中之一是 notebook的品質預測\n",
    "a. notebook品質對於排序/得分/過濾等等工作是有幫助的。\n",
    "b. 對多個部門都有益，可以在不同的部門中分享，因此決定此題目。\n",
    "\n",
    "- 提出正確答案是非常難的(下列為途中遇到的難題、挑戰)\n",
    "a. notebook品質是非常主觀的。\n",
    "b. 透過使用者表現尋找替代標準。\n",
    "c. 花費更多的時間在ground truth定義上, 而不是模型建立。\n",
    "d. 先前建立的資料分析結果就在此處幫助很大。\n",
    "e. 不斷與stakeholders(如產品決策者)確認目前假設是否正確。\n",
    "成功指標 (Success metrics)\n",
    "- 確認stakeholders\n",
    "a. 誰會去使用預測結果/會如何使用。\n",
    "b. 誰會投資我的模型。\n",
    "\n",
    "- 提出商業/產品指標\n",
    "a. 商業指標與ML指標有可能不一樣。\n",
    "b. ML指標以數學上合理為優先(如PR-AUC, Accuracy等等)\n",
    "c. 盡量使商業指標與數學指標方向一致，但這不容易，如: 有時候商業指標只專注於前10%資料的準確度。\n",
    "\n",
    "- 與stakeholders設定模型表現目標\n",
    "例: 如果模型表現指標達到x%, 將會替公司省下 $Y, 就launch模型。\n",
    "\n",
    "- 如果需要重新定義模型成功指標，不斷溝通確保雙方認知一致。\n",
    "BigQueryML使用經驗 (Experience with BigQueryML)\n",
    "簡單易用\n",
    "Data wrangling 在SQL內完成\n",
    "模型在SQL內建立完成\n",
    "疊代模型快速\n",
    "與TF hub相容，能夠從前者import TF 模型\n",
    "AutoML使用經驗 (Experience with AutoML)\n",
    "簡單易用\n",
    "節省開發時間\n",
    "特徵篩選: 特徵效果快速檢驗\n",
    "便宜: 3小時 $ 5\n",
    "最後沒有用在生產環境，但如果要在生產環境應不困難\n",
    "將ML導入生產環境中 (Putting it in production)\n",
    "\n",
    "system structure\n",
    "保持系統簡單，使得維護容易，第一次嘗試可以忽略部分components。\n",
    "資料同步\n",
    "特徵\n",
    "訓練\n",
    "監控\n",
    "推論/預測\n",
    "架構 (What we ended up with)\n",
    "\n",
    "Our choice, Kaggle\n",
    "\n",
    "Training\n",
    "\n",
    "Inference\n",
    "結論 (Conclusions)\n",
    "對比大公司，小公司面臨的挑戰是很不一樣的。\n",
    "即便是Kaggle這種以資料科學為主的公司，要說服去做ML專案也是不容易的，因為其成本/資料要求高。\n",
    "藉由耐心、經驗以及細心驗證，可以在成本與收益之間找到平衡。\n",
    "\"\"\"\n",
    "\n",
    "# 我自己的medium文章, 找單個字\n",
    "kw_model = KeyBERT(model='distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cjm3Q2iAm6JI",
    "outputId": "61cc2b26-3443-4159-a154-3fc60cce23e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science team centralized 將資料科學團隊壯大', 0.0352),\n",
       " ('資料科學團隊的建立會面臨著不一樣的挑戰 讓我們跟著 wendy 了解其在 kaggle', 0.1037),\n",
       " ('建立資料科學團隊 隨著資料科學在各領域的開發 應用持續升溫', 0.1345),\n",
       " ('建立專業分工的團隊是必經之路 資料科學團隊的建立會面臨著不一樣的挑戰 讓我們跟著 wendy 了解其在', 0.0759),\n",
       " ('隨著資料科學在各領域的開發 應用持續升溫 建立專業分工的團隊是必經之路', 0.1593)]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(doc, use_maxsum=True, top_n=5, keyphrase_ngram_range=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ieec-UknN7K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "關鍵字提取.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "138b5b358c4843779f9fe6e06a056aa0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38f2d80a2808438bb9fe16cca2f03d62": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40763304fcb745ea9c08ac1e5c80580f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5357ebb9024f41d9bc733a5c529257f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38f2d80a2808438bb9fe16cca2f03d62",
      "placeholder": "​",
      "style": "IPY_MODEL_f14034beff874cb194445e6a4928c016",
      "value": " 245M/245M [00:14&lt;00:00, 16.5MB/s]"
     }
    },
    "7b32383d01fe4695aecd70a4329dfc17": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae9402350f5643c583434f08925377e6",
       "IPY_MODEL_5357ebb9024f41d9bc733a5c529257f6"
      ],
      "layout": "IPY_MODEL_138b5b358c4843779f9fe6e06a056aa0"
     }
    },
    "a347223aaf374e5692cb5ab572fdc91e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae9402350f5643c583434f08925377e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a347223aaf374e5692cb5ab572fdc91e",
      "max": 244733649,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_40763304fcb745ea9c08ac1e5c80580f",
      "value": 244733649
     }
    },
    "f14034beff874cb194445e6a4928c016": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
