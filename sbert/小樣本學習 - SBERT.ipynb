{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:43:08.920895Z",
     "start_time": "2021-06-18T13:43:08.909897Z"
    }
   },
   "outputs": [],
   "source": [
    "import syntok.segmenter as segmenter\n",
    "\n",
    "document = \"\"\"\n",
    "SentenceTransformers Documentation\n",
    "SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\n",
    "\n",
    "You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\n",
    "\n",
    "The framework is based on PyTorch and Transformers and offers a large collection of pre-trained models tuned for various tasks. Further, it is easy to fine-tune your own models.\n",
    "\n",
    "Installation\n",
    "You can install it using pip:\n",
    "\n",
    "pip install -U sentence-transformers\n",
    "We recommand Python 3.6 or higher, and at least PyTorch 1.6.0. See installation for further installation options, especially if you want to use a GPU.\n",
    "\n",
    "Usage\n",
    "The usage is as simple as:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:43:37.216151Z",
     "start_time": "2021-06-18T13:43:37.130151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Transformers Documentation Sentence Transformers is a Python framework for state of the art sentence , text and image embeddings . \n",
      "The initial work is described in our paper Sentence BERT : Sentence Embeddings using Siamese BERT Networks . \n",
      "\n",
      "\n",
      "\n",
      "You can use this framework to compute sentence / text embeddings for more than 100 languages . \n",
      "These embeddings can then be compared e.g . with cosine similarity to find sentences with a similar meaning . \n",
      "This can be useful for semantic textual similar , semantic search , or paraphrase mining . \n",
      "\n",
      "\n",
      "\n",
      "The framework is based on Py Torch and Transformers and offers a large collection of pre trained models tuned for various tasks . \n",
      "Further , it is easy to fine tune your own models . \n",
      "\n",
      "\n",
      "\n",
      "Installation You can install it using pip : \n",
      "\n",
      "\n",
      "\n",
      "pip install - U sentence transformers We recommand Python 3.6 or higher , and at least Py Torch 1.6.0 . See installation for further installation options , especially if you want to use a GPU . \n",
      "\n",
      "\n",
      "\n",
      "Usage The usage is as simple as : \n",
      "\n",
      "\n",
      "\n",
      "from sentence transformers import Sentence Transformer model = Sentence Transformer ( 'paraphrase distilroberta base v1 ' ) \n",
      "\n",
      "\n",
      "\n",
      "# Our sentences we like to encode sentences = \n",
      "[ ' This framework generates embeddings for each input sentence ' , ' Sentences are passed as a list of string . ' , ' The quick brown fox jumps over the lazy dog . ' ] \n",
      "\n",
      "\n",
      "\n",
      "# Sentences are encoded by calling model.encode ( ) embeddings = model.encode ( sentences ) \n",
      "\n",
      "\n",
      "\n",
      "# Print the embeddings for sentence , embedding in zip ( sentences , embeddings ) : print ( \"Sentence : \" , sentence ) print ( \"Embedding : \" , embedding ) print ( \" \" )  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# choose the segmentation function you need/prefer\n",
    "\n",
    "for paragraph in segmenter.process(document):\n",
    "    for sentence in paragraph:\n",
    "        for token in sentence:\n",
    "            # roughly reproduce the input,\n",
    "            # except for hyphenated word-breaks\n",
    "            # and replacing \"n't\" contractions with \"not\",\n",
    "            # separating tokens by single spaces\n",
    "            print(token.value, end=' ')\n",
    "        print()  # print one sentence per line\n",
    "    print()  # separate paragraphs with newlines\n",
    "\n",
    "# for paragraph in segmenter.analyze(document):\n",
    "#     for sentence in paragraph:\n",
    "#         for token in sentence:\n",
    "#             # exactly reproduce the input\n",
    "#             # and do not remove \"imperfections\"\n",
    "#             print(token.spacing, token.value, sep='', end='')\n",
    "    print(\"\\n\")  # reinsert paragraph separators\n",
    "###### Two diffrent types of segmentator are available, we could use any one of them as per our convenience. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:52:25.843242Z",
     "start_time": "2021-06-18T13:49:40.801991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b3eeab08b545a8adc7537134f4a3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('stsb-bert-base')\n",
    "\n",
    "# sentences\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "# encode\n",
    "embedding_list = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T14:00:26.088018Z",
     "start_time": "2021-06-18T14:00:25.931498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0931991   0.58960193  0.79686326 ...  0.17124614 -1.0291953\n",
      "   0.16041785]\n",
      " [-0.34643695 -0.2202087   0.44292238 ...  0.14638452 -0.09052458\n",
      "   0.97680795]\n",
      " [-0.5632062   0.16794418 -0.4360822  ...  0.5827177   0.71271753\n",
      "   0.09079245]]\n",
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_list)  # ndarray\n",
    "print(embedding_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T14:05:38.090544Z",
     "start_time": "2021-06-18T14:05:38.075448Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aband\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# 我們前面已經先將document分成句子, 這邊透過加總回去得到document embedding\n",
    "\n",
    "# 轉成tensor, 才可以透過torch.mean\n",
    "embedding_list = torch.tensor(embedding_list)\n",
    "\n",
    "doc_1 = torch.mean(embedding_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T14:05:45.064120Z",
     "start_time": "2021-06-18T14:05:45.044158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T14:07:15.790158Z",
     "start_time": "2021-06-18T14:07:15.641132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_2 = ['With the help of this technique',\n",
    "    'we could generate more labeled data as well with decent accuracy',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "doc_2 = model.encode(sentences_2)\n",
    "doc_2 = torch.tensor(doc_2)\n",
    "doc_2 = torch.mean(doc_2, dim=0)\n",
    "doc_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T14:07:18.110134Z",
     "start_time": "2021-06-18T14:07:18.103134Z"
    }
   },
   "outputs": [],
   "source": [
    "distance = util.pytorch_cos_sim(doc_1, doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T14:07:20.404016Z",
     "start_time": "2021-06-18T14:07:20.386017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4183]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit6893c7013b164b1189a865dcaea9fb2f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
