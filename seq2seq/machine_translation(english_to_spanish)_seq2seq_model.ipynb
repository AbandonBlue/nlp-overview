{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine-translation(english-to-spanish)-seq2seq-model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 英文轉西班牙文翻譯模型 - 以seq2seq Transformer 實現\n",
        "### 介紹:\n",
        "- 將會學到:\n",
        "    1. keras layer 去向量化 text data\n",
        "    2. 實現 TransformerEncoder、TransformerDecoder 以及 PositionalEmbedding layer.\n",
        "    3. 準備 seq2seq data 給模型訓練\n",
        "    4. 使用訓練過的模型是產生從未看過的輸入句子翻譯結果。(seq2seq inference)"
      ],
      "metadata": {
        "id": "CcGEoel6e_E4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zzYCYL3bciqp"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 下載資料"
      ],
      "metadata": {
        "id": "IKT5r5zOf8Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pkz9UGxf6eY",
        "outputId": "59f3da34-653b-4945-bd42-bac2c8e9f280"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrayuDqvgXpR",
        "outputId": "ce476571-6b35-4831-a01d-38c7f3caa97e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.keras/datasets/spa-eng/spa.txt')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 解析資料\n",
        "每一行有包含英文句子以及對應的西班牙句子。英文句子是 source sequence 而 西班牙 則是 target sequence。現在需要把 \"[start]\" token、\"[end]\" token 各加在 西班牙句子的前後頭，當作辨識符。"
      ],
      "metadata": {
        "id": "oBqur6iWiryT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split('\\n')[:-1]\n",
        "\n",
        "text_pairs = []         # (eng, spa)\n",
        "for line in lines:\n",
        "    eng, spa = line.split('\\t')\n",
        "    spa = \"[start] \" + spa + ' [end]'\n",
        "    text_pairs.append((eng, spa))\n",
        "\n",
        "\n",
        "# 看看長得如何:\n",
        "for i in range(5):\n",
        "    print(text_pairs[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbJhXkU7gfRd",
        "outputId": "da7fd15f-8a66-4923-8dbc-8b007324e50a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Go.', '[start] Ve. [end]')\n",
            "('Go.', '[start] Vete. [end]')\n",
            "('Go.', '[start] Vaya. [end]')\n",
            "('Go.', '[start] Váyase. [end]')\n",
            "('Hi.', '[start] Hola. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "現在將資料切分成 train/validation/test"
      ],
      "metadata": {
        "id": "9Uyc2xRGkJjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)      # 打亂資料，因為可以從上面明顯的看到資料有一些規律\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples+num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples+num_val_samples:]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH9ifK-_jQGT",
        "outputId": "7d2818f1-dea3-4de3-a24f-8112f4e393ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 向量化 text data\n",
        "將會使用 2個 TextVectorization layer 將 text data 向量化(1 for eng, 1 for spa)，也就是說把原始的文字資料轉化成 int seqs，其每一個int代表 index of a word in vocabulary。\n",
        "\n",
        "英文的用預測的標準清除即可，透過空白分開；而spa 需要客製化，將標點符號去除清單加入\"¿\"。(spa 特有)。\n",
        "\n",
        "但在真實的應用中，我們可能不選擇去除標點符號。"
      ],
      "metadata": {
        "id": "UN-DZkS_lEAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", '')\n",
        "strip_chars = strip_chars.replace(\"]\", '')\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(strip_chars), \"\")\n",
        "\n",
        "# vocab_size 都用一樣只是剛好，可以各自設定。\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, \n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length + 1, # why?\n",
        "    standardize=custom_standardization\n",
        ")\n",
        "\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "# fit 差不多的意思\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "metadata": {
        "id": "6wQ-Oi92lBz_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下來，將會將其組成 datasets.\n",
        "\n",
        "每一個訓練步，模型將會找到 predict target word N+1 (持續) 透過使用 source sentence and the target words 0-N(前面已確認的word)。\n",
        "\n",
        "因此，訓練資料集將會不斷產生 (inputs, targets):\n",
        "- inputs: \n",
        "    - 是一個python dict，key=\"encoder_inputs\";key=\"decoder_inputs\"，可以說使用0-N的word去預測N+1的words\n",
        "- target: \n",
        "    - target sentence offset by one step，提供了下一個將要預測的words in the target sentence"
      ],
      "metadata": {
        "id": "3tpDynAzok5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\n",
        "        \"encoder_inputs\": eng,\n",
        "        \"decoder_inputs\": spa[:, :-1] # 0-N的概念\n",
        "        }\n",
        "        ,spa[:, 1:]  \n",
        "    )\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)       # 處理成dict\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "TsIx8K0poU15"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "快速看一下資料集是否正確:\n"
      ],
      "metadata": {
        "id": "td6mZ0Iqq_N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs[encoder_inputs].shape: {inputs['encoder_inputs'].shape}\")\n",
        "    print(f\"inputs[decoder_inputs].shape: {inputs['decoder_inputs'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUXnNzWEq9qc",
        "outputId": "e528cf95-d49e-4305-adae-1800b31bd25c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[encoder_inputs].shape: (64, 20)\n",
            "inputs[decoder_inputs].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 建立模型\n",
        "我們的 seq2seq Transformer 模型包含了 Encoder、Decoder 部分，要讓模型意識到句子的順序訊息則透過PositionalEmbedding layer，詳細可以看 Attention is all you need 論文。\n",
        "\n",
        "source sentence 將會 傳入 Encoder，其將會產生一個新的 representation of it. 這個新的 representation 將會傳給 Decoder，與 target sequence (0-N)一起。 Decoder 將會去預測 **下一個** words in the target sequence (N+1 and beyond).\n",
        "\n",
        "一個關鍵讓上述可以成功地點在於，我們用了 **causal masking**。Decoder 會一次看整個句子，我們必須確保 Decoder 只利用了 0-N 的 target tokens 去預測 N+，不然將會用到我們不該看到的tokens。"
      ],
      "metadata": {
        "id": "wQBZaXPFtO2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim,\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation='relu'), \n",
        "             layers.Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "    \n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis,], dtype='int32')    # 配合attention, 但我對於維度沒有到非常清楚。\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask,\n",
        "        )\n",
        "        # short-cut\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length,\n",
        "            output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_sie = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, \n",
        "            key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, \n",
        "            key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "    \n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        # 要詳細看看\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype='int32')\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)"
      ],
      "metadata": {
        "id": "yLrccUaPrYyo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "組成 end-to-end model"
      ],
      "metadata": {
        "id": "GWSFpRExzqjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "metadata": {
        "id": "EWJtm8CrzpHA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練模型\n",
        "我們將會使用 accuracy 作為一個快速判斷的 metrics，去監控訓練、驗證成效。真正在翻譯任何上比較常去看BLEU scores 作為 metrics。\n",
        "\n",
        "因為訓練時長關係，先用epochs=1來繼續。"
      ],
      "metadata": {
        "id": "_3_lDNxu0qHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50i_K65i0Dt3",
        "outputId": "e94b19e3-fa6f-4d60-b91a-26466bbf7414"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding_2 (Positi  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
            " onalEmbedding)                                                                                   \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding_2[0][0]'] \n",
            " erEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19,960,216\n",
            "Trainable params: 19,960,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "1302/1302 [==============================] - 94s 68ms/step - loss: 1.3811 - accuracy: 0.5176 - val_loss: 1.1842 - val_accuracy: 0.5634\n",
            "Epoch 2/10\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.2155 - accuracy: 0.5713 - val_loss: 1.0954 - val_accuracy: 0.5966\n",
            "Epoch 3/10\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.1143 - accuracy: 0.6071 - val_loss: 1.0491 - val_accuracy: 0.6194\n",
            "Epoch 4/10\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.0595 - accuracy: 0.6313 - val_loss: 1.0247 - val_accuracy: 0.6323\n",
            "Epoch 5/10\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.0255 - accuracy: 0.6484 - val_loss: 1.0127 - val_accuracy: 0.6373\n",
            "Epoch 6/10\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.0002 - accuracy: 0.6625 - val_loss: 1.0038 - val_accuracy: 0.6449\n",
            "Epoch 7/10\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 0.9809 - accuracy: 0.6733 - val_loss: 0.9987 - val_accuracy: 0.6502\n",
            "Epoch 8/10\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.9643 - accuracy: 0.6831 - val_loss: 0.9973 - val_accuracy: 0.6500\n",
            "Epoch 9/10\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 0.9496 - accuracy: 0.6907 - val_loss: 1.0001 - val_accuracy: 0.6539\n",
            "Epoch 10/10\n",
            "1302/1302 [==============================] - 92s 70ms/step - loss: 0.9345 - accuracy: 0.6980 - val_loss: 0.9954 - val_accuracy: 0.6560\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8332a6ee10>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding test sentences\n",
        "終於到了最後階段，掩飾如何去把新輸入的英文語句去翻譯。我們將會把向量化的英語句子以及 target token \"[start]\" 然後不斷產生下一個句子，直到我們遇到 \"[end]\" 才停止。"
      ],
      "metadata": {
        "id": "k6u2Sfyx1K8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, '--->', translated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdmExsJS08zn",
        "outputId": "c6e940c5-2aa8-4025-8a6e-c8564d07ffdb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I didn't know you couldn't drive. ---> [start] no sabía que no podría conducir [end]\n",
            "Tom doesn't like to be kept waiting for a long time. ---> [start] a tom no le gusta estar esperando un rato [end]\n",
            "Does your father get home early? ---> [start] tu padre se [UNK] pronto [end]\n",
            "Bring food. ---> [start] [UNK] comida [end]\n",
            "I often catch colds. ---> [start] me tengo [UNK] [end]\n",
            "Tom frequently speaks with Mary. ---> [start] tom casi no habla con mary [end]\n",
            "Don't lie to Tom. ---> [start] no te [UNK] a tom [end]\n",
            "Tom works in a factory. ---> [start] tom trabaja en una fábrica [end]\n",
            "Tom won't answer. ---> [start] tom no va a responder [end]\n",
            "I frequently think about my mother who passed away. ---> [start] yo no creo que mi madre me ha estado lejos de mi madre [end]\n",
            "Tom suddenly felt like he was going to faint. ---> [start] tom se sintió que estaba [UNK] a [UNK] [end]\n",
            "Everything was exciting to me when I visited Spain for the first time. ---> [start] me [UNK] me fue cuando [UNK] la vez que [UNK] por primera vez [end]\n",
            "Please give me the letter. ---> [start] por favor dame la carta [end]\n",
            "He tends to lie. ---> [start] Él [UNK] a la culpa [end]\n",
            "She is wearing a brooch. ---> [start] lleva un lleva una [UNK] [end]\n",
            "He called off the trip. ---> [start] Él me vino el viaje [end]\n",
            "Only Tom knows. ---> [start] solo tom sabe [end]\n",
            "Do you plan to continue working until 10:00? ---> [start] tú [UNK] tu trabajo hasta las seis [end]\n",
            "Don't get left behind. ---> [start] no se te dejes [end]\n",
            "I wasn't unhappy. ---> [start] no era feliz [end]\n",
            "Is that the real reason? ---> [start] es esa persona de la razón [end]\n",
            "The older we become, the worse our memory gets. ---> [start] cuanto más viejo nos estabas peor [end]\n",
            "You should've told me you needed money. ---> [start] deberías [UNK] que no te deberías dinero [end]\n",
            "My wife spends money as if I were the richest man in town. ---> [start] mi esposa hace lo más dinero como más alto de la ciudad en el ciudad [end]\n",
            "He attends the same school that I do. ---> [start] Él [UNK] lo mismo al colegio que yo [end]\n",
            "I'll go and meet him, if it's necessary. ---> [start] voy a comprar si él es necesario necesario [end]\n",
            "Tell Tom I'm in a meeting. ---> [start] dime a tom en una reunión [end]\n",
            "What time do you open? ---> [start] a qué hora te [UNK] [end]\n",
            "Take a good look. ---> [start] [UNK] una buena gente [end]\n",
            "The international situation is becoming grave. ---> [start] la situación [UNK] está en el que está en [UNK] [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "krXSFpPY2KI7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}