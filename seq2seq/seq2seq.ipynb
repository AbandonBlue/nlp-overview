{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiQgrs0CNkEY"
   },
   "source": [
    "## [seq2seq](https://keras.io/examples/nlp/lstm_seq2seq/)\n",
    "- 起因:\n",
    "    - 從line engineering學到這是它們embedding的其中一個, 我也剛好都沒有機會去實作\n",
    "    - 這是toy example的機器翻譯版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T09:17:45.882051Z",
     "start_time": "2021-03-01T09:17:29.453141Z"
    },
    "id": "3p-ulz6cNkEj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T09:20:01.838085Z",
     "start_time": "2021-03-01T09:20:01.833090Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqspN8ppNkEk",
    "outputId": "e80ee1ff-9bb1-4b4f-92c3-9edfbfc013f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  fra-eng.zip',\n",
       " '  inflating: _about.txt              ',\n",
       " '  inflating: fra.txt                 ']"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!curl -O http://www.manythings.org/anki/fra-eng.zip\r\n",
    "!!unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ef2W3FOvuh"
   },
   "source": [
    "#### Configuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PlwOf8BnNkEo"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\r\n",
    "epochs = 100\r\n",
    "latent_dim = 256\r\n",
    "num_samples = 10000\r\n",
    "data_path = 'fra.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wrPx7mOO_-8"
   },
   "source": [
    "#### 資料準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIotqrW1O-Ay",
    "outputId": "db44e9c9-19f2-4952-8fbd-f80f1c2b1ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# 向量化資料\r\n",
    "\r\n",
    "input_texts = []\r\n",
    "target_texts = []\r\n",
    "input_characters = set()            # 用來~\r\n",
    "target_characters = set()\r\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\r\n",
    "    lines = f.read().split('\\n')\r\n",
    "for line in lines[:min(num_samples, len(lines)-1)]:\r\n",
    "    input_text, target_text, _ = line.split('\\t')       # 可以觀察fra.txt檔案, 後面的檔案不是我們要的!\r\n",
    "    # 參考範例使用 \"\\t\" 當作答案的開頭, \"\\n\" 當作結束, 我使用\"beg\", \"ens\", 好像不行, 因為這是character level\r\n",
    "    target_text = '\\t' + target_text + '\\n'\r\n",
    "    input_texts.append(input_text)\r\n",
    "    target_texts.append(target_text)\r\n",
    "    for char in input_text:\r\n",
    "        if char not in input_characters:\r\n",
    "            input_characters.add(char)\r\n",
    "    for char in target_text:\r\n",
    "        if char not in target_characters:\r\n",
    "            target_characters.add(char)\r\n",
    "\r\n",
    "# 排序以及token大小\r\n",
    "input_characters = sorted(list(input_characters))\r\n",
    "target_characters = sorted(list(target_characters))\r\n",
    "num_encoder_tokens = len(input_characters)\r\n",
    "num_decoder_tokens = len(target_characters)\r\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\r\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\r\n",
    "print(\"Number of samples:\", len(input_texts))\r\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\r\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\r\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\r\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\r\n",
    "\r\n",
    "# token mapping ---> char:token index\r\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\r\n",
    "target_token_index = dict([char, i] for i, char in enumerate(target_characters))\r\n",
    "\r\n",
    "# 要做one-hot\r\n",
    "encoder_input_data = np.zeros(\r\n",
    "    shape=(len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32'\r\n",
    ")\r\n",
    "decoder_input_data = np.zeros(\r\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\r\n",
    ")\r\n",
    "decoder_target_data = np.zeros(\r\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\r\n",
    ")\r\n",
    "# 將對應出現位置放上1, one-hot的概念\r\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\r\n",
    "    for t, char in enumerate(input_text):\r\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\r\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0    # ?\r\n",
    "    for t, char in enumerate(target_text):\r\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\r\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\r\n",
    "        if t > 0:\r\n",
    "            # decoder_target_data will be ahead by one timestep\r\n",
    "            # and will not include the start character.\r\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\r\n",
    "        decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\r\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncBrCUlZY_Ao",
    "outputId": "e8556b8f-261e-4f8c-eb2e-ccf760b2b1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 15, 71)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoder_input_data.shape)     # (num_samples, seq_len, embedding_size(one-hot))\r\n",
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-XwTOzpZZvh",
    "outputId": "994a3125-0628-4a47-8322-2a88343427fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 59, 93)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data.shape)         # (num_samples, seq_len, embedding_size)\r\n",
    "print(decoder_input_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbtvWeMLT-lH"
   },
   "source": [
    "#### 建立model\r\n",
    "- 這裡應該可以包裝成一個Model, 但好像也沒需要...前面那些encoder可以在init那邊發生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "POp_HPtZTtn1"
   },
   "outputs": [],
   "source": [
    "# 定義輸入序列(原始)並處理\r\n",
    "encoder_inputs = tf.keras.Input(shape=(None, num_encoder_tokens))\r\n",
    "encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)       # 會return state\r\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\r\n",
    "\r\n",
    "# encoder_outputs是我們不需要的, 我們只需要keep states\r\n",
    "encoder_states = [state_h, state_c]\r\n",
    "\r\n",
    "# 設定decoder, 使用 `encoder_states` 當作初始狀態.\r\n",
    "decoder_inputs = tf.keras.Input(shape=(None, num_decoder_tokens))\r\n",
    "\r\n",
    "# 設定我們的decoder return full output sequences\r\n",
    "# 並且return 內部 states, 我們不使用return states in the training model, 但是會使用在推論階段\r\n",
    "# 因為訓練時可以teaching force!!\r\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\r\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\r\n",
    "decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax')\r\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\r\n",
    "\r\n",
    "# 定義模型\r\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPH4dAD7YaWf"
   },
   "source": [
    "#### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8bXQKAUYT4Y",
    "outputId": "e20bc3b9-7b24-4935-d711-8c9575141148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 8s 41ms/step - loss: 1.8119 - acc: 0.7036 - val_loss: 1.0776 - val_acc: 0.7055\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.9769 - acc: 0.7441 - val_loss: 0.9888 - val_acc: 0.7138\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.8305 - acc: 0.7731 - val_loss: 0.8549 - val_acc: 0.7704\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.7226 - acc: 0.8024 - val_loss: 0.7643 - val_acc: 0.7790\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 4s 31ms/step - loss: 0.6427 - acc: 0.8159 - val_loss: 0.7025 - val_acc: 0.7959\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.5914 - acc: 0.8300 - val_loss: 0.6657 - val_acc: 0.8068\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.5630 - acc: 0.8354 - val_loss: 0.6377 - val_acc: 0.8127\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.5390 - acc: 0.8425 - val_loss: 0.6179 - val_acc: 0.8193\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.5218 - acc: 0.8470 - val_loss: 0.6005 - val_acc: 0.8252\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.5022 - acc: 0.8520 - val_loss: 0.5876 - val_acc: 0.8264\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4921 - acc: 0.8543 - val_loss: 0.5766 - val_acc: 0.8297\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4794 - acc: 0.8578 - val_loss: 0.5612 - val_acc: 0.8363\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4643 - acc: 0.8622 - val_loss: 0.5541 - val_acc: 0.8397\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4546 - acc: 0.8652 - val_loss: 0.5417 - val_acc: 0.8431\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4484 - acc: 0.8670 - val_loss: 0.5327 - val_acc: 0.8447\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4325 - acc: 0.8713 - val_loss: 0.5239 - val_acc: 0.8472\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4244 - acc: 0.8738 - val_loss: 0.5183 - val_acc: 0.8488\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4141 - acc: 0.8769 - val_loss: 0.5078 - val_acc: 0.8518\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.4024 - acc: 0.8804 - val_loss: 0.5068 - val_acc: 0.8520\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3976 - acc: 0.8816 - val_loss: 0.4983 - val_acc: 0.8542\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3874 - acc: 0.8842 - val_loss: 0.4923 - val_acc: 0.8570\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3830 - acc: 0.8856 - val_loss: 0.4886 - val_acc: 0.8582\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3726 - acc: 0.8889 - val_loss: 0.4849 - val_acc: 0.8570\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3628 - acc: 0.8916 - val_loss: 0.4816 - val_acc: 0.8591\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3553 - acc: 0.8934 - val_loss: 0.4747 - val_acc: 0.8611\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3502 - acc: 0.8949 - val_loss: 0.4724 - val_acc: 0.8630\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3392 - acc: 0.8990 - val_loss: 0.4665 - val_acc: 0.8649\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3320 - acc: 0.9005 - val_loss: 0.4679 - val_acc: 0.8642\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3270 - acc: 0.9016 - val_loss: 0.4646 - val_acc: 0.8650\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3169 - acc: 0.9051 - val_loss: 0.4606 - val_acc: 0.8665\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3126 - acc: 0.9066 - val_loss: 0.4612 - val_acc: 0.8672\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.3047 - acc: 0.9086 - val_loss: 0.4577 - val_acc: 0.8687\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 4s 31ms/step - loss: 0.3026 - acc: 0.9099 - val_loss: 0.4587 - val_acc: 0.8688\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2908 - acc: 0.9127 - val_loss: 0.4539 - val_acc: 0.8691\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2884 - acc: 0.9137 - val_loss: 0.4555 - val_acc: 0.8698\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2782 - acc: 0.9164 - val_loss: 0.4546 - val_acc: 0.8714\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2729 - acc: 0.9180 - val_loss: 0.4528 - val_acc: 0.8711\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2682 - acc: 0.9194 - val_loss: 0.4484 - val_acc: 0.8735\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2638 - acc: 0.9206 - val_loss: 0.4499 - val_acc: 0.8734\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2545 - acc: 0.9237 - val_loss: 0.4513 - val_acc: 0.8734\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 4s 31ms/step - loss: 0.2466 - acc: 0.9262 - val_loss: 0.4473 - val_acc: 0.8752\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2427 - acc: 0.9273 - val_loss: 0.4532 - val_acc: 0.8731\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2396 - acc: 0.9281 - val_loss: 0.4515 - val_acc: 0.8741\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2330 - acc: 0.9298 - val_loss: 0.4535 - val_acc: 0.8755\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2298 - acc: 0.9305 - val_loss: 0.4539 - val_acc: 0.8754\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2225 - acc: 0.9327 - val_loss: 0.4569 - val_acc: 0.8746\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2175 - acc: 0.9346 - val_loss: 0.4572 - val_acc: 0.8754\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2091 - acc: 0.9371 - val_loss: 0.4614 - val_acc: 0.8747\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.2058 - acc: 0.9382 - val_loss: 0.4606 - val_acc: 0.8758\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1985 - acc: 0.9402 - val_loss: 0.4657 - val_acc: 0.8744\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1953 - acc: 0.9411 - val_loss: 0.4685 - val_acc: 0.8750\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1912 - acc: 0.9420 - val_loss: 0.4674 - val_acc: 0.8756\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1863 - acc: 0.9436 - val_loss: 0.4731 - val_acc: 0.8747\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1978 - acc: 0.9395 - val_loss: 0.4655 - val_acc: 0.8765\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1832 - acc: 0.9443 - val_loss: 0.4722 - val_acc: 0.8757\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1755 - acc: 0.9468 - val_loss: 0.4776 - val_acc: 0.8755\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1716 - acc: 0.9482 - val_loss: 0.4820 - val_acc: 0.8756\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1670 - acc: 0.9496 - val_loss: 0.4879 - val_acc: 0.8748\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1613 - acc: 0.9513 - val_loss: 0.4874 - val_acc: 0.8760\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1562 - acc: 0.9525 - val_loss: 0.4931 - val_acc: 0.8756\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1524 - acc: 0.9537 - val_loss: 0.4950 - val_acc: 0.8756\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1491 - acc: 0.9552 - val_loss: 0.4971 - val_acc: 0.8754\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1454 - acc: 0.9560 - val_loss: 0.5011 - val_acc: 0.8752\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1422 - acc: 0.9569 - val_loss: 0.5051 - val_acc: 0.8756\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1407 - acc: 0.9575 - val_loss: 0.5116 - val_acc: 0.8752\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1361 - acc: 0.9588 - val_loss: 0.5103 - val_acc: 0.8759\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1321 - acc: 0.9602 - val_loss: 0.5184 - val_acc: 0.8761\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1271 - acc: 0.9613 - val_loss: 0.5207 - val_acc: 0.8751\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1244 - acc: 0.9623 - val_loss: 0.5264 - val_acc: 0.8749\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1219 - acc: 0.9630 - val_loss: 0.5280 - val_acc: 0.8747\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1209 - acc: 0.9635 - val_loss: 0.5332 - val_acc: 0.8751\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1155 - acc: 0.9648 - val_loss: 0.5449 - val_acc: 0.8732\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1123 - acc: 0.9658 - val_loss: 0.5464 - val_acc: 0.8747\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1106 - acc: 0.9662 - val_loss: 0.5471 - val_acc: 0.8734\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1079 - acc: 0.9671 - val_loss: 0.5544 - val_acc: 0.8736\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1061 - acc: 0.9678 - val_loss: 0.5576 - val_acc: 0.8739\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.1033 - acc: 0.9686 - val_loss: 0.5608 - val_acc: 0.8734\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0991 - acc: 0.9699 - val_loss: 0.5726 - val_acc: 0.8735\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0973 - acc: 0.9703 - val_loss: 0.5737 - val_acc: 0.8737\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 4s 31ms/step - loss: 0.0951 - acc: 0.9707 - val_loss: 0.5773 - val_acc: 0.8738\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0917 - acc: 0.9718 - val_loss: 0.5806 - val_acc: 0.8737\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0903 - acc: 0.9722 - val_loss: 0.5852 - val_acc: 0.8726\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0880 - acc: 0.9729 - val_loss: 0.5924 - val_acc: 0.8733\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0864 - acc: 0.9733 - val_loss: 0.5937 - val_acc: 0.8725\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0838 - acc: 0.9740 - val_loss: 0.6020 - val_acc: 0.8728\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0841 - acc: 0.9741 - val_loss: 0.6035 - val_acc: 0.8723\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 4s 31ms/step - loss: 0.0785 - acc: 0.9757 - val_loss: 0.6082 - val_acc: 0.8730\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0784 - acc: 0.9755 - val_loss: 0.6125 - val_acc: 0.8729\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0767 - acc: 0.9766 - val_loss: 0.6182 - val_acc: 0.8720\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0738 - acc: 0.9772 - val_loss: 0.6202 - val_acc: 0.8711\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0737 - acc: 0.9772 - val_loss: 0.6343 - val_acc: 0.8713\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0733 - acc: 0.9769 - val_loss: 0.6349 - val_acc: 0.8721\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 4s 33ms/step - loss: 0.0707 - acc: 0.9780 - val_loss: 0.6371 - val_acc: 0.8722\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0684 - acc: 0.9786 - val_loss: 0.6447 - val_acc: 0.8716\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0679 - acc: 0.9786 - val_loss: 0.6463 - val_acc: 0.8714\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0662 - acc: 0.9789 - val_loss: 0.6501 - val_acc: 0.8724\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0647 - acc: 0.9795 - val_loss: 0.6557 - val_acc: 0.8712\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0637 - acc: 0.9800 - val_loss: 0.6632 - val_acc: 0.8708\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0614 - acc: 0.9806 - val_loss: 0.6670 - val_acc: 0.8705\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 4s 32ms/step - loss: 0.0598 - acc: 0.9810 - val_loss: 0.6652 - val_acc: 0.8716\n"
     ]
    }
   ],
   "source": [
    "model.compile(\r\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['acc']\r\n",
    ")\r\n",
    "model.fit(\r\n",
    "    [encoder_input_data, decoder_input_data],\r\n",
    "    decoder_target_data,\r\n",
    "    batch_size=batch_size,\r\n",
    "    epochs=epochs,\r\n",
    "    validation_split=0.2,\r\n",
    ")\r\n",
    "model.save('seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI72BpCNbd7x",
    "outputId": "cf51baef-bda5-409b-e08e-d65ce52fc57a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None, 71)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None, 93)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 256), (None, 335872      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 256),  358400      input_6[0][0]                    \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 93)     23901       lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQzuWe0XZwWm"
   },
   "source": [
    "#### 推論、預測(沒有teaching force了)!\r\n",
    "1. encoder 輸入並且取回decoder state\r\n",
    "2. 執行一次decoder with initial state and \"start of seq\"符號. 達到輸出即為下一次的target token(因為沒辦法teaching force)\r\n",
    "3. 之後就有current state, current target token可以繼續"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-PjM3pcuY0Xw"
   },
   "outputs": [],
   "source": [
    "# 模型讀取\r\n",
    "model = tf.keras.models.load_model('seq2seq_model.h5')\r\n",
    "\r\n",
    "encoder_inputs = model.input[0]\r\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\r\n",
    "encoder_states = [state_h_enc, state_c_enc]\r\n",
    "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\r\n",
    "\r\n",
    "decoder_inputs = model.input[1]  # input_2\r\n",
    "decoder_state_input_h = tf.keras.Input(shape=(latent_dim,), name=\"input_3\")\r\n",
    "decoder_state_input_c = tf.keras.Input(shape=(latent_dim,), name=\"input_4\")\r\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\r\n",
    "decoder_lstm = model.layers[3]\r\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\r\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\r\n",
    ")\r\n",
    "decoder_states = [state_h_dec, state_c_dec]\r\n",
    "decoder_dense = model.layers[4]\r\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\r\n",
    "decoder_model = tf.keras.Model(\r\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\r\n",
    ")\r\n",
    "\r\n",
    "# 將輸入lookup回去人看得懂的語言\r\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\r\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\r\n",
    "\r\n",
    "def decode_sequence(input_seq):\r\n",
    "    # encoder 輸入得到state vectors\r\n",
    "    states_value = encoder_model.predict(input_seq)\r\n",
    "\r\n",
    "    # Generate empty target sequence of length 1.\r\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\r\n",
    "    # Populate the first character of target sequence with the start character.\r\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\r\n",
    "\r\n",
    "    # Sampling loop for a batch of sequences\r\n",
    "    # (to simplify, here we assume a batch of size 1).\r\n",
    "    stop_condition = False\r\n",
    "    decoded_sentence = \"\"\r\n",
    "    while not stop_condition:\r\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\r\n",
    "\r\n",
    "        # Sample a token\r\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\r\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\r\n",
    "        decoded_sentence += sampled_char\r\n",
    "\r\n",
    "        # Exit condition: either hit max length\r\n",
    "        # or find stop character.\r\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\r\n",
    "            stop_condition = True\r\n",
    "\r\n",
    "        # Update the target sequence (of length 1).\r\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\r\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\r\n",
    "\r\n",
    "        # Update states\r\n",
    "        states_value = [h, c]\r\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rXI4cPDc5R8",
    "outputId": "c8b4eaeb-09a0-430b-a5c6-52698df87ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Marche.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Marche.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Marche.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\r\n",
    "    # Take one sequence (part of the training set)\r\n",
    "    # for trying out decoding.\r\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\r\n",
    "    decoded_sentence = decode_sequence(input_seq)\r\n",
    "    print(\"-\")\r\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\r\n",
    "    print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKQKUj23dERF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "seq2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit6893c7013b164b1189a865dcaea9fb2f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
